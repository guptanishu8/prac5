{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "9,10,11.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nNWLFR5uqFq",
        "outputId": "8ef94d4f-221b-4ebd-8ec6-4c4e1b02aa49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "['ham' 'ham' 'spam' ... 'ham' 'ham' 'ham']\n",
            "['ham' 'ham' 'ham' ... 'ham' 'ham' 'ham']\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.87      1.00      0.93      1448\n",
            "        spam       0.71      0.02      0.04       224\n",
            "\n",
            "    accuracy                           0.87      1672\n",
            "   macro avg       0.79      0.51      0.49      1672\n",
            "weighted avg       0.85      0.87      0.81      1672\n",
            "\n",
            "accuracy_score:  0.867822966507177\n"
          ]
        }
      ],
      "source": [
        "#prac 9\n",
        "import nltk\n",
        "import re\n",
        "from nltk.stem import PorterStemmer\n",
        "import pandas as pd\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "sms_data=pd.read_csv(\"/content/spam.csv\",encoding='latin-1')\n",
        "stemming=PorterStemmer()\n",
        "corpus=[]\n",
        "for i in range(0,len(sms_data)):\n",
        "  s1=re.sub(r'[^a-zA-z]',repl=\"\",string=sms_data['v2'][i])\n",
        "  s1.lower()\n",
        "  s1=s1.split()\n",
        "  s1=[stemming.stem(word)for word in s1 if word not in set(stopwords.words('english'))]\n",
        "  s1=\"\".join(s1)\n",
        "  corpus.append(s1)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "countvectorizer=CountVectorizer()\n",
        "x=countvectorizer.fit_transform(corpus).toarray()\n",
        "print(x)\n",
        "y = sms_data['v1'].values\n",
        "print(y)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=2,stratify=y)\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "multinomialnb=MultinomialNB()\n",
        "multinomialnb.fit(x_train,y_train)\n",
        "multinomialnb.fit(x_train,y_train)\n",
        "\n",
        "y_pred=multinomialnb.predict(x_test)\n",
        "print(y_pred)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"accuracy_score: \",accuracy_score(y_test,y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10 speech tagging using spacy\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "sp=spacy.load('en_core_web_sm')\n",
        "sen=sp(u'I like to play football. I hated it in my childhood though.')\n",
        "print(sen.text)\n",
        "print(sen[7].pos_)\n",
        "print(sen[7].tag_)\n",
        "print(spacy.explain(sen[7].tag_))\n",
        "for word in sen:\n",
        "  print(f'{word.text:{12}},{word.pos_:{10}},{word.tag_:{8}},{spacy.explain(word.tag_)}')\n",
        "  sen=sp(u\"You can google it?\")\n",
        "  word=sen[2]\n",
        "\n",
        "  print(f'{word.text:{12}},{word.pos_:{10}},{word.tag_:{8}},{spacy.explain(word.tag_)}')\n",
        "  sen=sp(u\"You can search it on google\")\n",
        "  word=sen[5]\n",
        "  \n",
        "  print(f'{word.text:{12}},{word.pos_:{10}},{word.tag_:{8}},{spacy.explain(word.tag_)}')\n",
        "  num_pos=sen.count_by(spacy.attrs.POS)\n",
        "for k,v in sorted(num_pos.items()):\n",
        "  print(f'{k}.{sen.vocab[k].text:{8}}:{v}')\n",
        "sen=sp(u'I like to play football. I hated it in my childhood though.')\n",
        "displacy.serve(sen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAGfZwv01x0E",
        "outputId": "6376adb5-6719-4938-d64b-0f0a618c184e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I like to play football. I hated it in my childhood though.\n",
            "VERB\n",
            "VBD\n",
            "verb, past tense\n",
            "I           ,PRON      ,PRP     ,pronoun, personal\n",
            "google      ,VERB      ,VB      ,verb, base form\n",
            "google      ,PROPN     ,NNP     ,noun, proper singular\n",
            "like        ,VERB      ,VBP     ,verb, non-3rd person singular present\n",
            "google      ,VERB      ,VB      ,verb, base form\n",
            "google      ,PROPN     ,NNP     ,noun, proper singular\n",
            "to          ,PART      ,TO      ,infinitival \"to\"\n",
            "google      ,VERB      ,VB      ,verb, base form\n",
            "google      ,PROPN     ,NNP     ,noun, proper singular\n",
            "play        ,VERB      ,VB      ,verb, base form\n",
            "google      ,VERB      ,VB      ,verb, base form\n",
            "google      ,PROPN     ,NNP     ,noun, proper singular\n",
            "football    ,NOUN      ,NN      ,noun, singular or mass\n",
            "google      ,VERB      ,VB      ,verb, base form\n",
            "google      ,PROPN     ,NNP     ,noun, proper singular\n",
            ".           ,PUNCT     ,.       ,punctuation mark, sentence closer\n",
            "google      ,VERB      ,VB      ,verb, base form\n",
            "google      ,PROPN     ,NNP     ,noun, proper singular\n",
            "I           ,PRON      ,PRP     ,pronoun, personal\n",
            "google      ,VERB      ,VB      ,verb, base form\n",
            "google      ,PROPN     ,NNP     ,noun, proper singular\n",
            "hated       ,VERB      ,VBD     ,verb, past tense\n",
            "google      ,VERB      ,VB      ,verb, base form\n",
            "google      ,PROPN     ,NNP     ,noun, proper singular\n",
            "it          ,PRON      ,PRP     ,pronoun, personal\n",
            "google      ,VERB      ,VB      ,verb, base form\n",
            "google      ,PROPN     ,NNP     ,noun, proper singular\n",
            "in          ,ADP       ,IN      ,conjunction, subordinating or preposition\n",
            "google      ,VERB      ,VB      ,verb, base form\n",
            "google      ,PROPN     ,NNP     ,noun, proper singular\n",
            "my          ,DET       ,PRP$    ,pronoun, possessive\n",
            "google      ,VERB      ,VB      ,verb, base form\n",
            "google      ,PROPN     ,NNP     ,noun, proper singular\n",
            "childhood   ,NOUN      ,NN      ,noun, singular or mass\n",
            "google      ,VERB      ,VB      ,verb, base form\n",
            "google      ,PROPN     ,NNP     ,noun, proper singular\n",
            "though      ,ADV       ,RB      ,adverb\n",
            "google      ,VERB      ,VB      ,verb, base form\n",
            "google      ,PROPN     ,NNP     ,noun, proper singular\n",
            ".           ,PUNCT     ,.       ,punctuation mark, sentence closer\n",
            "google      ,VERB      ,VB      ,verb, base form\n",
            "google      ,PROPN     ,NNP     ,noun, proper singular\n",
            "85.ADP     :1\n",
            "95.PRON    :2\n",
            "96.PROPN   :1\n",
            "100.VERB    :2\n",
            "\n",
            "Using the 'dep' visualizer\n",
            "Serving on http://0.0.0.0:5000 ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#speech tagging using nltk\n",
        "import nltk\n",
        "from nltk.corpus import state_union\n",
        "nltk.download('punkt')\n",
        "nltk.download('state_union')\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "train_text=state_union.raw(\"2005-GWBush.txt\")\n",
        "sample_text=state_union.raw('2006-GWBush.txt')\n",
        "custom_set_tokenizer=PunktSentenceTokenizer(train_text)\n",
        "tokenized=custom_set_tokenizer.tokenize(sample_text)\n",
        "def process_content():\n",
        "  try:\n",
        "    for word in tokenized:\n",
        "      words=nltk.word_tokenize(i)\n",
        "      tagged=nltk.pos_tag(words)\n",
        "      print(tagged)\n",
        "  except Exception as e:\n",
        "    print(str(e))\n",
        "process_content()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dYEnnMz45Rd",
        "outputId": "db92e105-5358-444b-a92c-108f8621f91e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]   Package state_union is already up-to-date!\n",
            "expected string or bytes-like object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# usage of give and gave in penn tree ban\n",
        "import nltk\n",
        "nltk.download('treebank')\n",
        "def give(t):\n",
        "  return t.label()=='VP' and len(t)>2 and t[1].label()=='NP'and(t[2].label()=='PP-DTV' or t[2].label()=='NP') and('give' in t[0].leaves() or 'gave' in t[0].leaves())\n",
        "def sent(t):\n",
        "  return \"\".join(token for token in t.leaves() if token[0] not in '*-0')\n",
        "def print_node(t,width):\n",
        "  output=\"%s%s:%s/%s:%s\"%(sent(t[0]),t[1].label(),sent(t[1]),t[2].label(),sent(t[2]))\n",
        "  if len(output)>width:\n",
        "    output=output[:width]+\"......\"\n",
        "    print(output)\n",
        "for tree in nltk.corpus.treebank.parsed_sents():\n",
        "  for t in tree.subtrees(give):\n",
        "    print_node(t,72)\n"
      ],
      "metadata": {
        "id": "ezDf3-Rx6mZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ip7FqrJ89RfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import ViterbiParser\n",
        "from nltk import PCFG\n",
        "grammar=PCFG.fromstring('''\n",
        "NP -> NNS[0.5] |JJ NNS[0.3]|NP CC NP[0.2]\n",
        "NNS -> \"man\"[0.1]|\"women\"[0.2]|children[0.3]|NNS CC NNS[0.4]\n",
        "JJ -> \"old\"[0.4]|\"young\"[0.6]\n",
        "CC -> \"and\"[0.9]|\"or\"[0.1]\n",
        "''')\n",
        "print(grammar)\n",
        "parser=ViterbiParser(grammar)\n",
        "sen=\"old man and young women\".split()\n",
        "#text=nltk.tokenize.word_tokenize(sen)\n",
        "x=parser.parse(sen)\n",
        "for i in x:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "uB3Ptigi9vFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.parse import malt\n",
        "mp=malt.MaltParser('maltparser-1.7.2','engmalt.linear-1.7.mco')\n",
        "t=malt.parse_onr(\"I saw a bird from my window\".split()).tree\n",
        "print(t)\n",
        "t.draw()"
      ],
      "metadata": {
        "id": "T7srhdq-BCYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prac 11 mwe tokenizer\n",
        "from nltk.tokenize import MWETokenizer\n",
        "from nltk import sent_tokenize,word_tokenize\n",
        "s=\"Good cake costs Rs.1500\\kg in Mumbai. Please buy me one of them. \\n\\n Thanks\"\n",
        "mwe=MWETokenizer()\n",
        "for sent in sent_tokenize(s):\n",
        "  print(mwe.tokenize(word_tokenize(sent)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJim3fQV_AIn",
        "outputId": "586f4ee7-58a8-415d-ba54-e56b3b421623"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Good', 'cake', 'costs', 'Rs.1500\\\\kg', 'in', 'Mumbai', '.']\n",
            "['Please', 'buy', 'me', 'one', 'of', 'them', '.']\n",
            "['Thanks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalized web distance and word similarity\n",
        "!pip install textdistance\n",
        "import numpy as np\n",
        "import re \n",
        "import textdistance\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "texts=['Reliance supermarket','Reliance hypermarket','Reliance','Reliance down town','relianc market','Reliance downtown','Mumbai','Mumbai Hyper','Mumbai dxb','mumbai airport','k.m trading','K.M. Trading','KM. Trading']\n",
        "\n",
        "def normalize(text):\n",
        "  return re.sub('[^a-z0-9]+','',text.lower())\n",
        "def group_texts(texts,threshold=0.4):\n",
        "  normalized_texts=np.array([normalize(text)for text in texts])\n",
        "  distances=1-np.array([[textdistance.jaro_winkler(one,another)for one in normalized_texts]for another in normalized_texts])\n",
        "  clustering=AgglomerativeClustering(distance_threshold=threshold,affinity='precomputed',linkage='complete',n_clusters=None).fit(distances)\n",
        "  centers=dict()\n",
        "  for cluster_id in set(clustering.labels_):\n",
        "    index=clustering.labels_==cluster_id\n",
        "    centrality=distances[:,index][index].sum(axis=1)\n",
        "    centers[cluster_id]=normalized_texts[index][centrality.argmin()]\n",
        "  return[centers[i] for i in clustering.labels_]\n",
        "print(group_texts(texts))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS_S1fPAFMSE",
        "outputId": "3dcbdc4c-6921-4205-ca48-6ad55dc903ea"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textdistance in /usr/local/lib/python3.7/dist-packages (4.2.2)\n",
            "['reliance', 'reliance', 'reliance', 'reliance', 'reliance', 'reliance', 'mumbai', 'mumbai', 'mumbai', 'mumbai', 'kmtrading', 'kmtrading', 'kmtrading']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#word sense disambiguation\n",
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('wordnet')\n",
        "def get_first_sense(word,pos=None):\n",
        "  if pos:\n",
        "    synsets=wn.synsets(word,pos)\n",
        "  else:\n",
        "    synsets=wn.synsets(word)\n",
        "  return synsets[0]\n",
        "\n",
        "best_synset=get_first_sense(\"bank\")\n",
        "print(\"%s:%s\"%(best_synset.name,best_synset.definition))\n",
        "best_synset=get_first_sense(\"set\",'n')\n",
        "print(\"%s:%s\"%(best_synset.name,best_synset.definition))\n",
        "best_synset=get_first_sense(\"set\",'v')\n",
        "print(\"%s:%s\"%(best_synset.name,best_synset.definition))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9dszHWWNKqI",
        "outputId": "9411d0a2-17fc-41cb-daed-f6642734cbcd"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "<bound method Synset.name of Synset('bank.n.01')>:<bound method Synset.definition of Synset('bank.n.01')>\n",
            "<bound method Synset.name of Synset('set.n.01')>:<bound method Synset.definition of Synset('set.n.01')>\n",
            "<bound method Synset.name of Synset('put.v.01')>:<bound method Synset.definition of Synset('put.v.01')>\n"
          ]
        }
      ]
    }
  ]
}