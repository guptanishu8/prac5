{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6,7,8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQiMvwnXcJHc"
      },
      "outputs": [],
      "source": [
        "#prac6a\n",
        "import nltk\n",
        "from nltk import tag\n",
        "from nltk import chunk\n",
        "from nltk import tokenize\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')\n",
        "para = \"Hello! My name is dinesh. Today you'll be learning NLTK.\"\n",
        "sents = tokenize.sent_tokenize(para)\n",
        "print(\"\\nsentence tokenization\",sents)\n",
        "\n",
        "for i in range(len(sents)):\n",
        "  words=tokenize.word_tokenize(sents[i])\n",
        "  print(words)\n",
        "pos_tagged=[]\n",
        "for i in range(len(sents)):\n",
        "  tagged=tag.pos_tag(words) \n",
        "  pos_tagged.append(tagged)\n",
        "print(\"pos tags\",tagged)\n",
        "\n",
        "tree=[]\n",
        "for i in range(len(sents)):\n",
        "  tree.append(chunk.ne_chunk(pos_tagged[i]))\n",
        "print(tree)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#) Named Entity recognition using user defined text.\n",
        "import spacy\n",
        "sp=spacy.load('en_core_web_sm')\n",
        "para=(\"European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices\")\n",
        "doc=sp(para)\n",
        "print([(X.text,X.label_)for X in doc.ents])\n",
        "print(\"noun phtases\",[chunk.text for chunk in doc.noun_chunks])\n",
        "print(\"verb\",[token.lemma_ for token in doc if token.pos_==\"VERB\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wJg9nZahTUA",
        "outputId": "050ded2e-3af1-41a4-bb6b-a54bda38e6fa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('European', 'NORP'), ('Google', 'ORG'), ('$5.1 billion', 'MONEY'), ('Wednesday', 'DATE')]\n",
            "noun phtases ['European authorities', 'Google', 'a record', 'Wednesday', 'its power', 'the mobile phone market', 'the company', 'its practices']\n",
            "verb ['fine', 'abuse', 'order', 'alter']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Named Entity recognition with diagram using NLTK corpus â€“ treebank. \n",
        "import nltk\n",
        "nltk.download('treebank')\n",
        "from nltk.corpus import treebank_chunk\n",
        "treebank_chunk.tagged_sents()[0]\n",
        "treebank_chunk.chunked_sents()[0]\n",
        "treebank_chunk.chunked_sents()[0].draw()"
      ],
      "metadata": {
        "id": "H9YY1HyziOa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prac 7\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "grammar=nltk.CFG.fromstring('''\n",
        "S -> VP\n",
        "VP -> NP VP\n",
        "NP ->Det NP\n",
        "NP -> Singular Noun\n",
        "Det ->\"that\"\n",
        "VP -> \"Book\"\n",
        "Np -> \"flight\"\n",
        "''')\n",
        "\n",
        "text=\"Book that flight\"\n",
        "tokenized=word_tokenize(text)\n",
        "print(tokenized)\n",
        "\n",
        "parser=nltk.ChartParser(grammar)\n",
        "for tree in parser.parse(tokenized):\n",
        "  tree.draw()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ErHGueckyXH",
        "outputId": "680e9902-d4ea-44e1-9d25-e3afcef36f6e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['Book', 'that', 'flight']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7b\n",
        "#101+\n",
        "def FA(s):\n",
        "  if len(s)<3:\n",
        "    return \"Rejected\"\n",
        "  if s[0]==\"1\":\n",
        "    if s[1]==\"0\":\n",
        "      if s[2]==\"1\":\n",
        "        for i in range(3,len(s)):\n",
        "          if s[i]!= \"1\":\n",
        "            return \"Rejected\"\n",
        "          return \"Accepted\"\n",
        "        return \"Rejected\"\n",
        "      return \"Rejected\"\n",
        "    return \"Rejected\"\n",
        "input=['101','10111','11111','101000']\n",
        "for i in input:\n",
        "  print(FA(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWuVUu8dorho",
        "outputId": "b4453b48-1f81-4ee5-89fe-12fe9fb22463"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rejected\n",
            "Accepted\n",
            "Rejected\n",
            "Rejected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#(a+b)*bba\n",
        "def FA(s):\n",
        "  size=0\n",
        "  for i in s:\n",
        "    if i=='a' or i=='b':\n",
        "      size=size+1\n",
        "    else:\n",
        "      return \"Rejected\"\n",
        "  if size>=3:\n",
        "    if s[size-3]==\"b\":\n",
        "      if s[size-2]==\"b\":\n",
        "        if s[size-1]==\"a\":\n",
        "          return \"Accepted\"\n",
        "        return \"Rejected\"\n",
        "      return \"Rejected\"\n",
        "    return \"Rejected\"\n",
        "input=['bba','abbba','aabb','abbbaa']\n",
        "for i in input:\n",
        "  print(FA(i))\n"
      ],
      "metadata": {
        "id": "fSagKOsBpYyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "grammar=nltk.CFG.fromstring('''\n",
        "S -> VP NP\n",
        "PP -> P NP\n",
        "NP -> Det N|Det N PP|\"I\"\n",
        "VP -> V NP|VP PP\n",
        "Det -> \"a\"|\"my\"\n",
        "N -> 'bird'|'balcony'\n",
        "V -> 'saw'\n",
        "P -> 'in'\n",
        "''')\n",
        "\n",
        "text=\"I saw a bird in my balcony\"\n",
        "tokenized=word_tokenize(text)\n",
        "print(tokenized)\n",
        "\n",
        "parser=nltk.ChartParser(grammar)\n",
        "for tree in parser.parse(tokenized):\n",
        "  tree.draw()"
      ],
      "metadata": {
        "id": "abv92Ep_paWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#practical8\n",
        "# PorterStemmer\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer \n",
        "word_stemmer =PorterStemmer()\n",
        "print(word_stemmer.stem('writing'))\n",
        "\n",
        "\n",
        "#LancasterStemmer\n",
        "import nltk\n",
        "from nltk.stem import LancasterStemmer\n",
        "Lanc_stemmer = LancasterStemmer()\n",
        "print(Lanc_stemmer.stem('writing'))\n",
        "\n",
        "\n",
        "#RegexpStemmer\n",
        "import nltk\n",
        "from nltk.stem import RegexpStemmer\n",
        "Reg_stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "print(Reg_stemmer.stem('writing'))\n",
        "\n",
        "\n",
        "#SnowballStemmer\n",
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "english_stemmer = SnowballStemmer('english')\n",
        "print(english_stemmer.stem ('writing'))\n",
        "\n",
        "#WordNetLemmatizer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "nltk.download('wordnet')\n",
        "  \n",
        "lemmatizer =WordNetLemmatizer()\n",
        "print(\"word :\\tlemma\")\n",
        "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
        "print(\"corpora :\",lemmatizer.lemmatize(\"corpora\"))\n",
        "# a denotes adjective in \"pos\"\n",
        "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWxiNjqep0pl",
        "outputId": "d29c0f13-fdef-4977-950a-d0662a4ecb03"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "write\n",
            "writ\n",
            "writ\n",
            "write\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "word :\tlemma\n",
            "rocks : rock\n",
            "corpora : corpus\n",
            "better : good\n"
          ]
        }
      ]
    }
  ]
}