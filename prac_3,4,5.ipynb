{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prac 3,4,5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIb8i-fNO34q"
      },
      "outputs": [],
      "source": [
        "#3a\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "print(wordnet.synsets(\"computer\"))\n",
        "print(wordnet.synset('computer.n.01').definition())\n",
        "print(wordnet.synset('computer.n.01').examples())\n",
        "print(wordnet.lemma('buy.v.01.buy').antonyms())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3b study lemmas hypernyms hyponyms\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "print(wordnet.synsets(\"computer\"))\n",
        "print(wordnet.synset(\"computer.n.01\").lemma_names())\n",
        "\n",
        "for e in wordnet.synsets(\"computer\"):\n",
        "  print(f'{e}---> {e.lemma_names()}')\n",
        "\n",
        "print(wordnet.synset(\"computer.n.01\").lemmas())\n",
        "print(wordnet.lemma(\"computer.n.01.computing_machine\").synset())\n",
        "\n",
        "syn=wordnet.synset(\"computer.n.01\")\n",
        "print(syn.hyponyms)\n",
        "print([lemma.name()for synset in syn.hyponyms()for lemma in synset.lemmas()])\n",
        "vehicle = wordnet.synset('vehicle.n.01')\n",
        "car = wordnet.synset('car.n.01')\n",
        "print(car.lowest_common_hypernyms(vehicle))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mFu3P5OO5iP",
        "outputId": "747c0610-a4b5-4ac5-95c7-032f67f9d66c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[Synset('computer.n.01'), Synset('calculator.n.01')]\n",
            "['computer', 'computing_machine', 'computing_device', 'data_processor', 'electronic_computer', 'information_processing_system']\n",
            "Synset('computer.n.01')---> ['computer', 'computing_machine', 'computing_device', 'data_processor', 'electronic_computer', 'information_processing_system']\n",
            "Synset('calculator.n.01')---> ['calculator', 'reckoner', 'figurer', 'estimator', 'computer']\n",
            "[Lemma('computer.n.01.computer'), Lemma('computer.n.01.computing_machine'), Lemma('computer.n.01.computing_device'), Lemma('computer.n.01.data_processor'), Lemma('computer.n.01.electronic_computer'), Lemma('computer.n.01.information_processing_system')]\n",
            "Synset('computer.n.01')\n",
            "<bound method _WordNetObject.hyponyms of Synset('computer.n.01')>\n",
            "['analog_computer', 'analogue_computer', 'digital_computer', 'home_computer', 'node', 'client', 'guest', 'number_cruncher', 'pari-mutuel_machine', 'totalizer', 'totaliser', 'totalizator', 'totalisator', 'predictor', 'server', 'host', 'Turing_machine', 'web_site', 'website', 'internet_site', 'site']\n",
            "[Synset('vehicle.n.01')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prac 3d\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "syn1 = wordnet.synsets('football')\n",
        "syn2 = wordnet.synsets('soccer')\n",
        "\n",
        "for s1 in syn1:\n",
        "  for s2 in syn2:\n",
        "    print(\"Path similarity of: \")\n",
        "    print(s1, '(', s1.pos(), ')', '[', s1.definition(), ']')\n",
        "    print(s2, '(', s2.pos(), ')', '[', s2.definition(), ']')\n",
        "    print(\" is\", s1.path_similarity(s2))\n",
        "    print()"
      ],
      "metadata": {
        "id": "2-zS39UPRdtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "P4DWUanISH4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prac 3c\n",
        "from nltk.corpus import wordnet\n",
        "print( wordnet.synsets(\"active\"))\n",
        "print(wordnet.lemma('active.a.01.active').antonyms())"
      ],
      "metadata": {
        "id": "9MlS4S6VP8iP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "text=\"Yash likes to play football,however he is not too fond of tennis\"\n",
        "tokens=word_tokenize(text)\n",
        "token_wihout_sw=[word for word in tokens if word not in stopwords.words()]\n",
        "print(token_wihout_sw)\n",
        "\n",
        "all_stopwords=stopwords.words('english')\n",
        "all_stopwords.append(\"play\")\n",
        "\n",
        "token_wihout_sw=[word for word in tokens if word not in all_stopwords]\n",
        "print(token_wihout_sw)\n",
        "\n",
        "all_stopwords.remove(\"play\")\n",
        "token_wihout_sw=[word for word in tokens if word not in all_stopwords]\n",
        "print(token_wihout_sw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YK_2lkT7SWcs",
        "outputId": "21a908f1-ece9-41a8-f7e8-c021f92d4f4c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['Yash', 'likes', 'play', 'football', ',', 'however', 'fond', 'tennis']\n",
            "['Yash', 'likes', 'football', ',', 'however', 'fond', 'tennis']\n",
            "['Yash', 'likes', 'play', 'football', ',', 'however', 'fond', 'tennis']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "text=\"Yash likes to play football,however he is not too fond of tennis\"\n",
        "filtered=remove_stopwords(text)\n",
        "print(filtered)\n",
        "\n",
        "all_stopwords=gensim.parsing.preprocessing.STOPWORDS\n",
        "list={\"play\"}\n",
        "all_stopwords_gensim=STOPWORDS.union(list)\n",
        "token_wihout_sw=[word for word in tokens if word not in all_stopwords_gensim]\n",
        "print(token_wihout_sw)\n",
        "all_stopwords_gensim=STOPWORDS.difference(list)\n",
        "token_wihout_sw=[word for word in tokens if word not in all_stopwords_gensim]\n",
        "print(token_wihout_sw)"
      ],
      "metadata": {
        "id": "DKElPwi0Sl_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "\n",
        "all_stopwords = sp.Defaults.stop_words\n",
        "all_stopwords.add(\"play\")\n",
        "text = \"Yashesh likes to play football, however he is not too fond of tennis.\"\n",
        "text_tokens = word_tokenize(text)\n",
        "tokens_without_sw = [word for word in text_tokens if word not in all_stopwords]\n",
        "print(tokens_without_sw)\n",
        "\n",
        "all_stopwords = sp.Defaults.stop_words\n",
        "all_stopwords.remove('play')\n",
        "text = \"Yashesh likes to play football, however he is not too fond of tennis.\"\n",
        "text_tokens = word_tokenize(text)\n",
        "tokens_without_sw = [word for word in text_tokens if word not in all_stopwords]\n",
        "print(tokens_without_sw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqrhc98eTQP8",
        "outputId": "5c2feb05-234d-4a3f-da07-93bccfae4b87"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Yashesh', 'likes', 'football', ',', 'not', 'fond', 'tennis', '.']\n",
            "['Yashesh', 'likes', 'play', 'football', ',', 'not', 'fond', 'tennis', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prac5\n",
        "text = \"\"\" This tool is an a beta stage. Alexa developers can use Get Metrics API to\n",
        "seamlessly analyse metric. It also supports custom skill model, prebuilt Flash Briefing\n",
        "model, and the Smart Home Skill API. You can use this tool for creation of monitors,\n",
        "alarms, and dashboards that spotlight changes. The release of these three tools will\n",
        "enable developers to create visual rich skills for Alexa devices with screens. Amazon\n",
        "describes these tools as the collection of tech and tools for creating visually rich and\n",
        "interactive voice experiences. \"\"\" \n",
        "data = text.split('.')\n",
        "for i in data:\n",
        "  print (i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IET7FmtNVB00",
        "outputId": "058cbd79-2249-4b55-f3a7-62527e25c511"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " This tool is an a beta stage\n",
            " Alexa developers can use Get Metrics API to\n",
            "seamlessly analyse metric\n",
            " It also supports custom skill model, prebuilt Flash Briefing\n",
            "model, and the Smart Home Skill API\n",
            " You can use this tool for creation of monitors,\n",
            "alarms, and dashboards that spotlight changes\n",
            " The release of these three tools will\n",
            "enable developers to create visual rich skills for Alexa devices with screens\n",
            " Amazon\n",
            "describes these tools as the collection of tech and tools for creating visually rich and\n",
            "interactive voice experiences\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tk = RegexpTokenizer('\\s+', gaps = True)\n",
        "\n",
        "str = \"I love to study Natural Language Processing in Python\"\n",
        "\n",
        "tokens = tk.tokenize(str)"
      ],
      "metadata": {
        "id": "MISIOHb_VKe5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Create a string input\n",
        "str = \"I love to study Natural Language Processing in Python\"\n",
        "# Use tokenize method\n",
        "print(word_tokenize(str))"
      ],
      "metadata": {
        "id": "8lTRdiDxVe8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.blank(\"en\")\n",
        "str = \"I love to study Natural Language Processing in Python\"\n",
        "doc = nlp(str)\n",
        "words = [word.text for word in doc]\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvA-bQtlVl6K",
        "outputId": "acd81465-933a-4099-92a8-5bc42d4a9f3f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'love', 'to', 'study', 'Natural', 'Language', 'Processing', 'in', 'Python']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "str = \"I love to study Natural Language Processing in Python\"\n",
        "\n",
        "tokens = text_to_word_sequence(str)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nZDqqE5V00e",
        "outputId": "54511c7a-7364-4427-d514-f9138fc84baa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'love', 'to', 'study', 'natural', 'language', 'processing', 'in', 'python']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import tokenize\n",
        "\n",
        "str = \"I love to study Natural Language Processing in Python\"\n",
        "\n",
        "list(tokenize(str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srMZrHS6V-yU",
        "outputId": "360d1b23-f692-4e0e-db47-9a1579e8d7f9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'love',\n",
              " 'to',\n",
              " 'study',\n",
              " 'Natural',\n",
              " 'Language',\n",
              " 'Processing',\n",
              " 'in',\n",
              " 'Python']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prac5\n",
        "!pip install torch==1.3.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install inltk\n",
        "!pip install tornado==4.5.3\n",
        "from inltk.inltk import setup\n",
        "setup('hi')\n",
        "from inltk.inltk import tokenize\n",
        "hindi_text = \"\"\"प्राकृ ितक भाषा सीखना बिहु ितलचस्प है।\"\"\"\n",
        "\n",
        "tokenize(hindi_text, \"hi\")"
      ],
      "metadata": {
        "id": "pOgdNFwRWvkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.3.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install inltk\n",
        "!pip install tornado==4.5.3\n",
        "from inltk.inltk import setup\n",
        "\n",
        "from inltk.inltk import get_similar_sentences\n",
        "\n",
        "output = get_similar_sentences('मैं आज बिहु खु हूं', 5,'hi') \n",
        "print(output)"
      ],
      "metadata": {
        "id": "Jj65foUYYSu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8Gqvc4nflo3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install inltk\n",
        "!pip install tornado==4.5.3\n",
        "from inltk.inltk import setup\n",
        "setup('gu')\n",
        "from inltk.inltk import identify_language\n",
        "#Identify the Lnaguage of given text\n",
        "identify_language('બીના કાપડયા')"
      ],
      "metadata": {
        "id": "S6Rtf07eaFHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WxZ9hpI9lqGe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}